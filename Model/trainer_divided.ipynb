{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\abubabu\\\\Documents\\\\GitHub\\\\synthetic-population-simulation\\\\NHTS Data Parser\\\\HH Data Parser\\\\trippub.csv\"\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "columns_to_keep = ['TRPACCMP','HOUSEID', 'PERSONID',\"STRTTIME\",\"ENDTIME\",'TRVLCMIN', 'R_SEX', 'R_AGE', 'WRKCOUNT','HHVEHCNT', 'HHSIZE','EDUC','WHYTRP1S','HHFAMINC']\n",
    "final_data = pd.read_csv(file_path,usecols=columns_to_keep)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateAge(age):\n",
    "    if age <=5:\n",
    "        return 1\n",
    "    elif age <= 18:\n",
    "        return 2\n",
    "    elif age <= 25:\n",
    "        return 3\n",
    "    elif age < 60:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# get the symbolic name of the gender from numerical value\n",
    "def translateSex(sex):\n",
    "    if sex == 1:\n",
    "        return 'male'\n",
    "    else:\n",
    "        return 'female'\n",
    "\n",
    "# get the race categories from numerical race values\n",
    "def translateRace(race):\n",
    "    if race == 1:\n",
    "        return 1 #white\n",
    "    elif race == 2:\n",
    "        return 2 #black\n",
    "    elif race == 4 and race ==6:\n",
    "        return 3 #native\n",
    "    elif race == 3:\n",
    "        return 6 #asian\n",
    "    elif race == 5:\n",
    "        return 7 #'pacific-islander'\n",
    "    else:\n",
    "        return 8 #'other'\n",
    "    \n",
    "def translateInc(inc):\n",
    "    if inc>=10: return 10\n",
    "    if inc<1: return 0\n",
    "    return inc   \n",
    "\n",
    "def translateTSum(ts):\n",
    "    if ts==1: return 1 #Home\n",
    "    elif ts==10: return 2 #work\n",
    "    elif ts==20: return 3 #school\n",
    "    elif ts==30: return 4 #medical\n",
    "    elif ts==40: return 5 #shopping\n",
    "    elif ts==50: return 6 #social\n",
    "    elif ts==70: return 7 #transport\n",
    "    elif ts==80: return 8 #meals\n",
    "    elif ts==97: return 9 #other\n",
    "    else : return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "final_data['R_AGE'] = final_data.apply(lambda row: translateAge(row['R_AGE']), axis=1)\n",
    "# final_data['R_RACE'] = final_data.apply(lambda row: translateRace(row['R_RACE']), axis=1)\n",
    "final_data['HHFAMINC'] = final_data.apply(lambda row: translateInc(row['HHFAMINC']), axis=1)\n",
    "final_data['WHYTRP1S'] = final_data.apply(lambda row: translateTSum(row['WHYTRP1S']), axis=1)\n",
    "# final_data['STRTTIME'] = final_data['STRTTIME'].apply(lambda x: datetime.time(hour=x // 100, minute=x % 100))\n",
    "# final_data['ENDTIME'] = final_data['ENDTIME'].apply(lambda x: datetime.time(hour=x // 100, minute=x % 100))\n",
    "# final_data['TRVLCMIN'] = final_data['TRVLCMIN'].apply(lambda x: datetime.time(hour=x // 100, minute=x % 100))\n",
    "\n",
    "#KORLAM ABAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STRTTIME\n",
       "1700    17077\n",
       "1600    15211\n",
       "1500    14534\n",
       "1000    14212\n",
       "1100    13626\n",
       "        ...  \n",
       "439         1\n",
       "226         1\n",
       "221         1\n",
       "346         1\n",
       "341         1\n",
       "Name: count, Length: 1397, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['STRTTIME'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(738857, 11)\n",
      "(738857,)\n",
      "(923572, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "X = final_data.drop(columns=['WHYTRP1S','HOUSEID', 'PERSONID'],axis=1)\n",
    "# X = final_data.drop(columns=['WHYTRP1S','HOUSEID', 'PERSONID','STRTTIME','ENDTIME','TRVLCMIN'],axis=1)\n",
    "y = final_data['WHYTRP1S']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_train_scaled.shape)\n",
    "print(y_train.shape)\n",
    "print(X.shape)\n",
    "num_classes =10\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_train[\u001b[39m0\u001b[39;49m]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m   1006\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m   1009\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   1010\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1115\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.11390705, -1.07630199, -0.50266214, ..., -0.1048354 ,\n",
       "        -0.6809178 ,  0.85813544],\n",
       "       [ 0.74398245,  0.68145211, -0.50266214, ...,  0.95195306,\n",
       "         0.54369188,  0.85813544],\n",
       "       [-0.30897759, -0.35594918, -0.35093417, ..., -2.21841231,\n",
       "        -2.51783233,  0.85813544],\n",
       "       ...,\n",
       "       [-1.4789332 , -1.53322481, -0.50266214, ..., -0.1048354 ,\n",
       "        -0.6809178 ,  0.85813544],\n",
       "       [ 1.66824738,  1.60229146, -0.50266214, ..., -0.1048354 ,\n",
       "         1.15599672, -0.94759634],\n",
       "       [ 1.7969425 ,  1.75382199, -0.1992062 , ..., -1.16162385,\n",
       "         0.54369188,  0.85813544]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "23090/23090 [==============================] - 16s 673us/step - loss: 1.4764 - accuracy: 0.4893 - val_loss: 1.4515 - val_accuracy: 0.4989\n",
      "Epoch 2/3\n",
      "23090/23090 [==============================] - 16s 673us/step - loss: 1.4390 - accuracy: 0.5032 - val_loss: 1.4283 - val_accuracy: 0.5073\n",
      "Epoch 3/3\n",
      "23090/23090 [==============================] - 15s 660us/step - loss: 1.4278 - accuracy: 0.5076 - val_loss: 1.4192 - val_accuracy: 0.5104\n",
      "5773/5773 [==============================] - 3s 489us/step - loss: 1.4192 - accuracy: 0.5104\n",
      "Test loss: 1.419206976890564\n",
      "Test accuracy: 0.5104349851608276\n"
     ]
    }
   ],
   "source": [
    "# Create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=3, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "# model.fit(X_train, y, epochs=3, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.06%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Create a DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 27.71%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing SVM\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'SVM': SVC(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "}\n",
    "y_train_oh = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_oh = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "# Train and evaluate each classifier\n",
    "accuracies = {}\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print('doing',clf_name)\n",
    "    if clf_name in ['Logistic Regression', 'SVM']:\n",
    "        # For Logistic Regression and SVM, we use y_train and y_test without one-hot encoding\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies[clf_name] = accuracy\n",
    "    else:\n",
    "       \n",
    "        # For other classifiers, we use y_train and y_test with one-hot encoding\n",
    "        clf.fit(X_train, y_train_oh)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)  # Convert one-hot encoded predictions back to 1D array\n",
    "\n",
    "        accuracy = accuracy_score(y_test_oh, y_pred)\n",
    "        accuracies[clf_name] = accuracy\n",
    "    print(clf_name,accuracy)\n",
    "\n",
    "# Plot the accuracies in a graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(accuracies.keys(), accuracies.values(), color='skyblue')\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Classifier Accuracy Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"C:\\\\Users\\\\abubabu\\\\Documents\\\\GitHub\\\\synthetic-population-simulation\\\\Model\\\\v4_cp.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STRTTIME</th>\n",
       "      <th>ENDTIME</th>\n",
       "      <th>TRVLCMIN</th>\n",
       "      <th>HHSIZE</th>\n",
       "      <th>HHVEHCNT</th>\n",
       "      <th>HHFAMINC</th>\n",
       "      <th>WRKCOUNT</th>\n",
       "      <th>R_AGE</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>R_SEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>1100</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>1100</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>1100</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1100</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>1100</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STRTTIME  ENDTIME  TRVLCMIN  HHSIZE  HHVEHCNT  HHFAMINC  WRKCOUNT  R_AGE  \\\n",
       "0      1000     1100        60       3       0.0        10       0.0      5   \n",
       "1      1000     1100        60       3       0.0        10       0.0      5   \n",
       "2      1000     1100        60       3       0.0        10       0.0      5   \n",
       "3      1000     1100        60       2       3.0        10       3.0      4   \n",
       "4      1000     1100        60       2       3.0        10       3.0      4   \n",
       "\n",
       "   EDUC  R_SEX  \n",
       "0     3      2  \n",
       "1     3      2  \n",
       "2     3      2  \n",
       "3     1      2  \n",
       "4     1      2  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_predict=pd.read_csv(\"C:\\\\Users\\\\abubabu\\\\Documents\\\\GitHub\\\\synthetic-population-simulation\\Model\\\\shundordata.csv\")\n",
    "data_to_predict=data_to_predict.drop(['Unnamed: 0'],axis=1)\n",
    "# data_to_predict=data_to_predict.drop(['Unnamed: 0','STRTTIME','ENDTIME','TRVLCMIN'],axis=1)\n",
    "data_to_predict.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 578us/step\n"
     ]
    }
   ],
   "source": [
    "data_to_predict = scaler.transform(data_to_predict)\n",
    "predictions = model.predict(data_to_predict)\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('C:\\\\Users\\\\abubabu\\\\Documents\\\\GitHub\\\\synthetic-population-simulation\\\\Activity assignment\\\\scaler_filename.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the filename for the CSV file\n",
    "filename = 'data.csv'\n",
    "\n",
    "# Save the NumPy array to the CSV file\n",
    "np.savetxt(filename, predictions, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data.txt'\n",
    "np.savetxt(filename, predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     160854\n",
       "10    138405\n",
       "7     132897\n",
       "8     110961\n",
       "5     100756\n",
       "4      70939\n",
       "9      64388\n",
       "3      59997\n",
       "2      30177\n",
       "1      29858\n",
       "0      24340\n",
       "Name: HHFAMINC, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['HHFAMINC'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
