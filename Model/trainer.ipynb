{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\abubabu\\\\Documents\\\\GitHub\\\\synthetic-population-simulation\\\\NHTS Data Parser\\\\HH Data Parser\\\\trippub.csv\"\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "columns_to_keep = ['HOUSEID', 'PERSONID',\"STRTTIME\",\"ENDTIME\",'TRVLCMIN', 'R_SEX', 'R_AGE', 'WRKCOUNT','HHVEHCNT', 'HHSIZE','EDUC','WHYTRP1S','HHFAMINC']\n",
    "final_data = pd.read_csv(file_path,usecols=columns_to_keep)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateAge(age):\n",
    "    if age <=5:\n",
    "        return 1\n",
    "    elif age <= 18:\n",
    "        return 2\n",
    "    elif age <= 25:\n",
    "        return 3\n",
    "    elif age < 60:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# get the symbolic name of the gender from numerical value\n",
    "def translateSex(sex):\n",
    "    if sex == 1:\n",
    "        return 'male'\n",
    "    else:\n",
    "        return 'female'\n",
    "\n",
    "# get the race categories from numerical race values\n",
    "def translateRace(race):\n",
    "    if race == 1:\n",
    "        return 1 #white\n",
    "    elif race == 2:\n",
    "        return 2 #black\n",
    "    elif race == 4 and race ==6:\n",
    "        return 3 #native\n",
    "    elif race == 3:\n",
    "        return 6 #asian\n",
    "    elif race == 5:\n",
    "        return 7 #'pacific-islander'\n",
    "    else:\n",
    "        return 8 #'other'\n",
    "    \n",
    "def translateInc(inc):\n",
    "    if inc>=10: return 10\n",
    "    if inc<1: return 0\n",
    "    return inc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_data['R_AGE'] = final_data.apply(lambda row: translateAge(row['R_AGE']), axis=1)\n",
    "# final_data['R_RACE'] = final_data.apply(lambda row: translateRace(row['R_RACE']), axis=1)\n",
    "final_data['HHFAMINC'] = final_data.apply(lambda row: translateInc(row['HHFAMINC']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOUSEID</th>\n",
       "      <th>PERSONID</th>\n",
       "      <th>STRTTIME</th>\n",
       "      <th>ENDTIME</th>\n",
       "      <th>TRVLCMIN</th>\n",
       "      <th>WHYTRP1S</th>\n",
       "      <th>HHSIZE</th>\n",
       "      <th>HHVEHCNT</th>\n",
       "      <th>HHFAMINC</th>\n",
       "      <th>WRKCOUNT</th>\n",
       "      <th>R_AGE</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>R_SEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000007</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1015</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30000007</td>\n",
       "      <td>1</td>\n",
       "      <td>1510</td>\n",
       "      <td>1530</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30000007</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>900</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30000007</td>\n",
       "      <td>2</td>\n",
       "      <td>1800</td>\n",
       "      <td>2030</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30000007</td>\n",
       "      <td>3</td>\n",
       "      <td>845</td>\n",
       "      <td>900</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30000007</td>\n",
       "      <td>3</td>\n",
       "      <td>1430</td>\n",
       "      <td>1445</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30000008</td>\n",
       "      <td>1</td>\n",
       "      <td>1115</td>\n",
       "      <td>1130</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30000008</td>\n",
       "      <td>1</td>\n",
       "      <td>2330</td>\n",
       "      <td>2340</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30000012</td>\n",
       "      <td>1</td>\n",
       "      <td>550</td>\n",
       "      <td>605</td>\n",
       "      <td>15</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30000012</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>715</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    HOUSEID  PERSONID  STRTTIME  ENDTIME  TRVLCMIN  WHYTRP1S  HHSIZE  \\\n",
       "0  30000007         1      1000     1015        15        20       3   \n",
       "1  30000007         1      1510     1530        20         1       3   \n",
       "2  30000007         2       700      900       120         1       3   \n",
       "3  30000007         2      1800     2030       150        10       3   \n",
       "4  30000007         3       845      900        15        20       3   \n",
       "5  30000007         3      1430     1445        15         1       3   \n",
       "6  30000008         1      1115     1130        15        10       2   \n",
       "7  30000008         1      2330     2340        10         1       2   \n",
       "8  30000012         1       550      605        15        50       1   \n",
       "9  30000012         1       700      715        15         1       1   \n",
       "\n",
       "   HHVEHCNT  HHFAMINC  WRKCOUNT  R_AGE  EDUC  R_SEX  \n",
       "0         5         7         1      5     3      2  \n",
       "1         5         7         1      5     3      2  \n",
       "2         5         7         1      5     3      1  \n",
       "3         5         7         1      5     3      1  \n",
       "4         5         7         1      4     2      2  \n",
       "5         5         7         1      4     2      2  \n",
       "6         4         8         2      4     5      1  \n",
       "7         4         8         2      4     5      1  \n",
       "8         2        10         1      4     5      2  \n",
       "9         2        10         1      4     5      2  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     160854\n",
       "10    138405\n",
       "7     132897\n",
       "8     110961\n",
       "5     100756\n",
       "4      70939\n",
       "9      64388\n",
       "3      59997\n",
       "2      30177\n",
       "1      29858\n",
       "0      24340\n",
       "Name: HHFAMINC, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['HHFAMINC'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(738857, 10)\n",
      "(738857, 9)\n",
      "(923572, 10)\n",
      "(923572,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming \"final_data\" is a pandas DataFrame containing your dataset\n",
    "# Make sure you have already preprocessed the data and removed any missing values.\n",
    "\n",
    "# Extracting the features (X) and target (y) from the dataset\n",
    "# X = final_data.drop(columns=['TRIPPURP','WHYFROM','HOUSEID', 'PERSONID',\"TDAYDATE\",'WRKCOUNT'],axis=1)\n",
    "X = final_data.drop(columns=['WHYTRP1S','HOUSEID', 'PERSONID'],axis=1)\n",
    "y = final_data['WHYTRP1S']\n",
    "\n",
    "# Preprocess 'R_AGE' and 'R_SEX' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "X['R_AGE'] = label_encoder.fit_transform(X['R_AGE'])\n",
    "X['R_SEX'] = label_encoder.fit_transform(X['R_SEX'])\n",
    "\n",
    "\n",
    "# Preprocess the target variable using LabelEncoder\n",
    "y_label_encoder = LabelEncoder()\n",
    "y_encoded = y_label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert the target variable to one-hot encoded format\n",
    "num_classes = len(y_label_encoder.classes_)\n",
    "y_one_hot = pd.get_dummies(y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train=X.astype('float64')\n",
    "num_classes\n",
    "# Standardize the features (optional, but often recommended for neural networks)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "# X_test_scaled.shape\n",
    "print(X_train_scaled.shape)\n",
    "print(y_train.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0  1  2  3  4  5  6  7  8\n",
       "1  0  0  0  0  0  0  0  0    318777\n",
       "0  0  0  0  1  0  0  0  0    184126\n",
       "   1  0  0  0  0  0  0  0    110590\n",
       "   0  0  0  0  1  0  0  0    100284\n",
       "               0  0  1  0     72327\n",
       "                  1  0  0     56377\n",
       "      1  0  0  0  0  0  0     43397\n",
       "      0  0  0  0  0  0  1     20910\n",
       "         1  0  0  0  0  0     16784\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36286562, 0.3819415 , 0.01157982, ..., 0.16666667, 0.25      ,\n",
       "        0.76923077],\n",
       "       [0.69944892, 0.70156846, 0.01157982, ..., 0.08333333, 0.08333333,\n",
       "        0.92307692],\n",
       "       [0.50869012, 0.51292921, 0.01571547, ..., 0.08333333, 0.41666667,\n",
       "        0.53846154],\n",
       "       ...,\n",
       "       [0.29673591, 0.29885545, 0.01157982, ..., 0.16666667, 0.08333333,\n",
       "        0.76923077],\n",
       "       [0.86689275, 0.86901229, 0.01157982, ..., 0.16666667, 0.25      ,\n",
       "        1.        ],\n",
       "       [0.89020772, 0.89656634, 0.01985112, ..., 0.33333333, 0.16666667,\n",
       "        0.92307692]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 9) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\abubabu\\Documents\\GitHub\\synthetic-population-simulation\\Model\\trainer.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abubabu/Documents/GitHub/synthetic-population-simulation/Model/trainer.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abubabu/Documents/GitHub/synthetic-population-simulation/Model/trainer.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/abubabu/Documents/GitHub/synthetic-population-simulation/Model/trainer.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_scaled, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test_scaled, y_test))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abubabu/Documents/GitHub/synthetic-population-simulation/Model/trainer.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# model.fit(X_train, y, epochs=3, batch_size=32, validation_split=0.2)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abubabu/Documents/GitHub/synthetic-population-simulation/Model/trainer.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abubabu/Documents/GitHub/synthetic-population-simulation/Model/trainer.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abubabu/Documents/GitHub/synthetic-population-simulation/Model/trainer.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test_scaled, y_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filevodog23p.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 9) are incompatible\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add layers to the model\n",
    "# model.add(Dense(64, activation='relu', input_shape=(input_shape,)))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=3, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "# model.fit(X_train, y, epochs=3, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 64)                576       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,854\n",
      "Trainable params: 2,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(923572, 8)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"C:\\\\Users\\\\abubabu\\\\Documents\\\\GitHub\\\\synthetic-population-simulation\\\\Model\\\\v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25870665620009203\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.36      0.26      0.30     38092\n",
      "           2       0.29      0.19      0.23     38881\n",
      "           3       0.23      0.15      0.18     21993\n",
      "           4       0.36      0.28      0.31     23445\n",
      "           5       0.43      0.33      0.37     62299\n",
      "\n",
      "   micro avg       0.36      0.26      0.30    184715\n",
      "   macro avg       0.28      0.20      0.23    184715\n",
      "weighted avg       0.35      0.26      0.30    184715\n",
      " samples avg       0.26      0.26      0.26    184715\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abubabu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming you have X_train, X_test, y_train, y_test from the train-test split\n",
    "\n",
    "# Create the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "dt_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = dt_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "23090/23090 [==============================] - 24s 1ms/step - loss: 1.6864 - accuracy: 0.4146 - val_loss: 1.6198 - val_accuracy: 0.4348 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "23090/23090 [==============================] - 24s 1ms/step - loss: 1.6571 - accuracy: 0.4230 - val_loss: 1.6073 - val_accuracy: 0.4415 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "23090/23090 [==============================] - 24s 1ms/step - loss: 1.6502 - accuracy: 0.4253 - val_loss: 1.6033 - val_accuracy: 0.4377 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "23090/23090 [==============================] - 24s 1ms/step - loss: 1.6450 - accuracy: 0.4261 - val_loss: 1.6025 - val_accuracy: 0.4406 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "23090/23090 [==============================] - 25s 1ms/step - loss: 1.6411 - accuracy: 0.4263 - val_loss: 1.5954 - val_accuracy: 0.4421 - lr: 0.0010\n",
      "5773/5773 [==============================] - 3s 603us/step - loss: 1.5954 - accuracy: 0.4421\n",
      "Test loss: 1.5953700542449951\n",
      "Test accuracy: 0.44205397367477417\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Assuming \"final_data\" is a pandas DataFrame containing your dataset\n",
    "# Make sure you have already preprocessed the data and removed any missing values.\n",
    "\n",
    "# Extracting the features (X) and target (y) from the dataset\n",
    "# X = final_data.drop(columns=['TRIPPURP', 'WHYFROM'])\n",
    "# y = final_data['WHYFROM']\n",
    "\n",
    "# # Preprocess 'R_AGE' and 'R_SEX' using LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# X['R_AGE'] = label_encoder.fit_transform(X['R_AGE'])\n",
    "# X['R_SEX'] = label_encoder.fit_transform(X['R_SEX'])\n",
    "\n",
    "# # Preprocess the target variable using LabelEncoder\n",
    "# y_label_encoder = LabelEncoder()\n",
    "# y_encoded = y_label_encoder.fit_transform(y)\n",
    "\n",
    "# # Convert the target variable to one-hot encoded format\n",
    "# num_classes = len(y_label_encoder.classes_)\n",
    "# y_one_hot = pd.get_dummies(y_encoded)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the features (optional, but often recommended for neural networks)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))  # Adding dropout for regularization\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Adding dropout for regularization\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)  # You can experiment with different learning rates\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Adding learning rate scheduler to reduce learning rate during training\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=5, batch_size=32, validation_data=(X_test_scaled, y_test), callbacks=[reduce_lr])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (738857, 6) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\abubabu\\Documents\\GitHub\\synthetic-population-simulation\\NHTS Data Parser - Rahat\\HH Data Parser\\final_dataset_parser.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abubabu/Documents/GitHub/synthetic-population-simulation/NHTS%20Data%20Parser%20-%20Rahat/HH%20Data%20Parser/final_dataset_parser.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_squared_error, r2_score\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abubabu/Documents/GitHub/synthetic-population-simulation/NHTS%20Data%20Parser%20-%20Rahat/HH%20Data%20Parser/final_dataset_parser.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/abubabu/Documents/GitHub/synthetic-population-simulation/NHTS%20Data%20Parser%20-%20Rahat/HH%20Data%20Parser/final_dataset_parser.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_scaled, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abubabu/Documents/GitHub/synthetic-population-simulation/NHTS%20Data%20Parser%20-%20Rahat/HH%20Data%20Parser/final_dataset_parser.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abubabu/Documents/GitHub/synthetic-population-simulation/NHTS%20Data%20Parser%20-%20Rahat/HH%20Data%20Parser/final_dataset_parser.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:1138\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[1;32m-> 1138\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1139\u001b[0m     X,\n\u001b[0;32m   1140\u001b[0m     y,\n\u001b[0;32m   1141\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1142\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[0;32m   1143\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1144\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1145\u001b[0m )\n\u001b[0;32m   1146\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1090\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1071\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1072\u001b[0m     )\n\u001b[0;32m   1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1075\u001b[0m     X,\n\u001b[0;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1088\u001b[0m )\n\u001b[1;32m-> 1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39;49mmulti_output, y_numeric\u001b[39m=\u001b[39;49my_numeric, estimator\u001b[39m=\u001b[39;49mestimator)\n\u001b[0;32m   1092\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1094\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1111\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1110\u001b[0m     estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m-> 1111\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1112\u001b[0m     _assert_all_finite(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, estimator_name\u001b[39m=\u001b[39mestimator_name)\n\u001b[0;32m   1113\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1156\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m   1147\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1148\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA column-vector y was passed when a 1d array was\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1149\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected. Please change the shape of y to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   1153\u001b[0m         )\n\u001b[0;32m   1154\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mravel(y)\n\u001b[1;32m-> 1156\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1157\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39my should be a 1d array, got an array of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(shape)\n\u001b[0;32m   1158\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (738857, 6) instead."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23090/23090 [==============================] - 28s 1ms/step - loss: 1.8700 - accuracy: 0.3617 - val_loss: 1.6723 - val_accuracy: 0.4276 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "23090/23090 [==============================] - 28s 1ms/step - loss: 1.7341 - accuracy: 0.3992 - val_loss: 1.6517 - val_accuracy: 0.4265 - lr: 9.0484e-04\n",
      "Epoch 3/10\n",
      "23090/23090 [==============================] - 28s 1ms/step - loss: 1.7137 - accuracy: 0.4037 - val_loss: 1.6431 - val_accuracy: 0.4260 - lr: 8.1873e-04\n",
      "Epoch 4/10\n",
      "23090/23090 [==============================] - 27s 1ms/step - loss: 1.7028 - accuracy: 0.4066 - val_loss: 1.6377 - val_accuracy: 0.4284 - lr: 7.4082e-04\n",
      "Epoch 5/10\n",
      "23090/23090 [==============================] - 27s 1ms/step - loss: 1.6950 - accuracy: 0.4094 - val_loss: 1.6338 - val_accuracy: 0.4292 - lr: 6.7032e-04\n",
      "Epoch 6/10\n",
      "23090/23090 [==============================] - 28s 1ms/step - loss: 1.6917 - accuracy: 0.4102 - val_loss: 1.6309 - val_accuracy: 0.4297 - lr: 6.0653e-04\n",
      "Epoch 7/10\n",
      "23090/23090 [==============================] - 31s 1ms/step - loss: 1.6878 - accuracy: 0.4107 - val_loss: 1.6288 - val_accuracy: 0.4298 - lr: 5.4881e-04\n",
      "Epoch 8/10\n",
      "23090/23090 [==============================] - 26s 1ms/step - loss: 1.6844 - accuracy: 0.4124 - val_loss: 1.6273 - val_accuracy: 0.4300 - lr: 4.9659e-04\n",
      "Epoch 9/10\n",
      "23090/23090 [==============================] - 25s 1ms/step - loss: 1.6834 - accuracy: 0.4122 - val_loss: 1.6260 - val_accuracy: 0.4305 - lr: 4.4933e-04\n",
      "Epoch 10/10\n",
      "23090/23090 [==============================] - 28s 1ms/step - loss: 1.6809 - accuracy: 0.4131 - val_loss: 1.6251 - val_accuracy: 0.4312 - lr: 4.0657e-04\n",
      "5773/5773 [==============================] - 4s 659us/step - loss: 1.6251 - accuracy: 0.4312\n",
      "Test loss: 1.6251161098480225\n",
      "Test accuracy: 0.43118858337402344\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# # Assuming \"final_data\" is a pandas DataFrame containing your dataset\n",
    "# # Make sure you have already preprocessed the data and removed any missing values.\n",
    "\n",
    "# # Extracting the features (X) and target (y) from the dataset\n",
    "# X = final_data.drop(columns=['TRIPPURP', 'WHYFROM'])\n",
    "# y = final_data['WHYFROM']\n",
    "\n",
    "# # Preprocess 'R_AGE' and 'R_SEX' using LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# X['R_AGE'] = label_encoder.fit_transform(X['R_AGE'])\n",
    "# X['R_SEX'] = label_encoder.fit_transform(X['R_SEX'])\n",
    "\n",
    "# # Preprocess the target variable using LabelEncoder\n",
    "# y_label_encoder = LabelEncoder()\n",
    "# y_encoded = y_label_encoder.fit_transform(y)\n",
    "\n",
    "# # Convert the target variable to one-hot encoded format\n",
    "# num_classes = len(y_label_encoder.classes_)\n",
    "# y_one_hot = pd.get_dummies(y_encoded)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the features (optional, but often recommended for neural networks)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())  # Batch Normalization\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())  # Batch Normalization\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * np.exp(-epoch / 10)\n",
    "\n",
    "# Compile the model with SGD optimizer and learning rate scheduler\n",
    "optimizer = SGD(learning_rate=0.0)  # Use a placeholder learning rate, the scheduler will update it\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Set up learning rate scheduler\n",
    "lr_scheduler_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test), callbacks=[lr_scheduler_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"C:\\\\Users\\\\abubabu\\\\Documents\\\\GitHub\\\\synthetic-population-simulation\\\\Model\\\\v2.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
